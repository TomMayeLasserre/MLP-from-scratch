{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d06856f2",
   "metadata": {},
   "source": [
    "\n",
    "# Multilayer Perceptron (MLP)\n",
    "\n",
    "### Definition:\n",
    "\n",
    "An MLP is a type of feedforward neural network consisting of multiple layers of neurons, where each layer is fully connected to the next. Given an input vector $\\mathbf{x} \\in \\mathbb{R}^n$, the output of the MLP can be described mathematically as follows:\n",
    "\n",
    "### Input Layer:\n",
    "\n",
    "The input to the MLP is the vector $\\mathbf{x} = [x_1, x_2, \\dots, x_n]$. There is no computation done at this layer.\n",
    "\n",
    "### Hidden Layer(s):\n",
    "\n",
    "For a hidden layer $h$, the output $\\mathbf{h}^{(l)}$ is computed as:\n",
    "\n",
    "$$\\mathbf{h}^{(l)} = \\sigma(\\mathbf{W}^{(l)} \\mathbf{h}^{(l-1)} + \\mathbf{b}^{(l)})$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{h}^{(l-1)}$ is the input to the layer (or the output of the previous layer).\n",
    "- $\\mathbf{W}^{(l)} \\in \\mathbb{R}^{m_l \\times m_{l-1}}$ is the weight matrix.\n",
    "- $\\mathbf{b}^{(l)} \\in \\mathbb{R}^{m_l}$ is the bias vector.\n",
    "- $\\sigma$ is an activation function, typically $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ (sigmoid) or $\\sigma(x) = \\max(0, x)$ (ReLU).\n",
    "\n",
    "### Output Layer:\n",
    "\n",
    "The final layer computes the output as:\n",
    "\n",
    "$$\\mathbf{y} = \\sigma(\\mathbf{W}^{(L)} \\mathbf{h}^{(L-1)} + \\mathbf{b}^{(L)})$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{W}^{(L)} \\in \\mathbb{R}^{m_L \\times m_{L-1}}$ is the weight matrix for the output layer.\n",
    "- $\\mathbf{b}^{(L)} \\in \\mathbb{R}^{m_L}$ is the bias for the output layer.\n",
    "- $\\mathbf{y}$ represents the final output of the MLP.\n",
    "\n",
    "### Backpropagation:\n",
    "\n",
    "The weight updates during training are governed by the backpropagation algorithm. The gradients of the loss function $\\mathcal{L}(\\mathbf{y}, \\mathbf{t})$ (where $\\mathbf{t}$ is the true label) with respect to the weights are given by:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(l)}} = \\delta^{(l)} \\mathbf{h}^{(l-1)^T}$$\n",
    "\n",
    "Where:\n",
    "- $\\delta^{(l)} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}^{(l)}} \\cdot \\sigma'(\\mathbf{h}^{(l)})$ is the error term at layer $l$.\n",
    "- $\\sigma'(x)$ is the derivative of the activation function.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
